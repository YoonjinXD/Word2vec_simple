{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] mode partition\n",
      "ipykernel_launcher.py: error: the following arguments are required: partition\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from random import shuffle\n",
    "from collections import Counter\n",
    "import argparse\n",
    "\n",
    "\n",
    "def skipgram(centerWord, contextWord, inputMatrix, outputMatrix):\n",
    "################################  Input  ################################\n",
    "# centerWord : Index of a centerword (type:int)                         #\n",
    "# contextWord : Index of a contextword (type:int)                       #\n",
    "# inputMatrix : Weight matrix of input (type:torch.tesnor(V,D))         #\n",
    "# outputMatrix : Weight matrix of output (type:torch.tesnor(V,D))       #\n",
    "#########################################################################\n",
    "\n",
    "###############################  Output  ################################\n",
    "# loss : Loss value (type:torch.tensor(1))                              #\n",
    "# grad_in : Gradient of inputMatrix (type:torch.tensor(1,D))            #\n",
    "# grad_out : Gradient of outputMatrix (type:torch.tesnor(V,D))          #\n",
    "#########################################################################\n",
    "\n",
    "    loss = None\n",
    "    grad_in = None\n",
    "    grad_out = None\n",
    "\n",
    "    return loss, grad_in, grad_out\n",
    "\n",
    "def CBOW(centerWord, contextWords, inputMatrix, outputMatrix):\n",
    "################################  Input  ################################\n",
    "# centerWord : Index of a centerword (type:int)                         #\n",
    "# contextWords : Indices of contextwords (type:list(int))               #\n",
    "# inputMatrix : Weight matrix of input (type:torch.tesnor(V,D))         #\n",
    "# outputMatrix : Weight matrix of output (type:torch.tesnor(V,D))       #\n",
    "#########################################################################\n",
    "\n",
    "###############################  Output  ################################\n",
    "# loss : Loss value (type:torch.tensor(1))                              #\n",
    "# grad_in : Gradient of inputMatrix (type:torch.tensor(1,D))            #\n",
    "# grad_out : Gradient of outputMatrix (type:torch.tesnor(V,D))          #\n",
    "#########################################################################\n",
    "\n",
    "    loss = None\n",
    "    grad_in = None\n",
    "    grad_out = None\n",
    "\n",
    "    return loss, grad_in, grad_out\n",
    "\n",
    "\n",
    "def word2vec_trainer(train_seq, numwords, stats, mode=\"CBOW\", dimension=100, learning_rate=0.025, epoch=3):\n",
    "# train_seq : list(tuple(int, list(int))\n",
    "\n",
    "# Xavier initialization of weight matrices\n",
    "    W_in = torch.randn(numwords, dimension) / (dimension**0.5)\n",
    "    W_out = torch.randn(numwords, dimension) / (dimension**0.5)\n",
    "    i=0\n",
    "    losses=[]\n",
    "\n",
    "    print(\"# of training samples\")\n",
    "    if mode==\"CBOW\":\n",
    "    \tprint(len(train_seq))\n",
    "    elif mode==\"SG\":\n",
    "    \tprint(len(train_seq)*len(train_seq[0][1]))\n",
    "    print()\n",
    "\n",
    "    for _ in range(epoch):\n",
    "        #Random shuffle of training data\n",
    "        shuffle(train_seq)\n",
    "        #Training word2vec using SGD(Batch size : 1)\n",
    "        for center, contexts in train_seq:\n",
    "            i+=1\n",
    "            centerInd = center\n",
    "            contextInds = contexts\n",
    "            if mode==\"CBOW\":\n",
    "                L, G_in, G_out = CBOW(centerInd, contextInds, W_in, W_out)\n",
    "                \n",
    "                W_in[contextInds] -= learning_rate*G_in\n",
    "                W_out -= learning_rate*G_out\n",
    "\n",
    "                losses.append(L.item())\n",
    "            elif mode==\"SG\":\n",
    "            \tfor contextInd in contextInds:\n",
    "\t                L, G_in, G_out = skipgram(centerInd, contextInd, W_in, W_out)\n",
    "\t                \n",
    "\t                W_in[centerInd] -= learning_rate*G_in.squeeze()\n",
    "\t                W_out -= learning_rate*G_out\n",
    "\n",
    "\t                losses.append(L.item())\n",
    "            else:\n",
    "                print(\"Unkwnown mode : \"+mode)\n",
    "                exit()\n",
    "\n",
    "            if i%10000==0:\n",
    "            \tavg_loss=sum(losses)/len(losses)\n",
    "            \tprint(\"Loss : %f\" %(avg_loss,))\n",
    "            \tlosses=[]\n",
    "\n",
    "    return W_in, W_out\n",
    "\n",
    "def sim(testword, word2ind, ind2word, matrix):\n",
    "    length = (matrix*matrix).sum(1)**0.5\n",
    "    wi = word2ind[testword]\n",
    "    inputVector = matrix[wi].reshape(1,-1)/length[wi]\n",
    "    sim = (inputVector@matrix.t())[0]/length\n",
    "    values, indices = sim.squeeze().topk(5)\n",
    "    \n",
    "    print()\n",
    "    print(\"===============================================\")\n",
    "    print(\"The most similar words to \\\"\" + testword + \"\\\"\")\n",
    "    for ind, val in zip(indices,values):\n",
    "        print(ind2word[ind.item()]+\":%.3f\"%(val,))\n",
    "    print(\"===============================================\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Word2vec')\n",
    "    parser.add_argument('mode', metavar='mode', type=str,\n",
    "                        help='\"SG\" for skipgram, \"CBOW\" for CBOW')\n",
    "    parser.add_argument('part', metavar='partition', type=str,\n",
    "                        help='\"part\" if you want to train on a part of corpus, \"full\" if you want to train on full corpus')\n",
    "    args = parser.parse_args()\n",
    "    mode = args.mode\n",
    "    part = args.part\n",
    "\n",
    "\t#Load and preprocess corpus\n",
    "    print(\"loading...\")\n",
    "    if part==\"part\":\n",
    "        text = open('text8',mode='r').readlines()[0][:1000000] #Load a part of corpus for debugging\n",
    "    elif part==\"full\":\n",
    "        text = open('text8',mode='r').readlines()[0] #Load full corpus for submission\n",
    "    else:\n",
    "        print(\"Unknown argument : \" + part)\n",
    "        exit()\n",
    "\n",
    "    print(\"preprocessing...\")\n",
    "    corpus = text.split()\n",
    "    stats = Counter(corpus)\n",
    "    words = []\n",
    "    #Discard rare words\n",
    "    for word in corpus:\n",
    "        if stats[word]>4:\n",
    "            words.append(word)\n",
    "\n",
    "    freqtable = []\n",
    "    vocab = set(words)\n",
    "\n",
    "    #Give an index number to a word\n",
    "    w2i = {}\n",
    "    w2i[\" \"]=0\n",
    "    i = 1\n",
    "    for word in vocab:\n",
    "        w2i[word] = i\n",
    "        i+=1\n",
    "    i2w = {}\n",
    "    for k,v in w2i.items():\n",
    "        i2w[v]=k\n",
    "\n",
    "    #Frequency table for negative sampling\n",
    "    for k,v in stats.items():\n",
    "        f = int(v**0.75)\n",
    "        for _ in range(f):\n",
    "            if k in w2i.keys():\n",
    "                freqtable.append(w2i[k])\n",
    "\n",
    "    print(\"build training set...\")\n",
    "    #Make tuples of (centerword, contextwords) for training\n",
    "    train_set = []\n",
    "    window_size = 5\n",
    "    for j in range(len(words)):\n",
    "        if j<window_size:\n",
    "            contextlist = [0 for _ in range(window_size-j)] + [w2i[words[k]] for k in range(j)] + [w2i[words[j+k+1]] for k in range(window_size)]\n",
    "            train_set.append((w2i[words[j]],contextlist))\n",
    "        elif j>=len(words)-window_size:\n",
    "            contextlist = [w2i[words[j-k-1]] for k in range(window_size)] + [w2i[words[len(words)-k-1]] for k in range(len(words)-j-1)] + [0 for _ in range(j+window_size-len(words)+1)]\n",
    "            train_set.append((w2i[words[j]],contextlist))\n",
    "        else:\n",
    "            contextlist = [w2i[words[j-k-1]] for k in range(window_size)] + [w2i[words[j+k+1]] for k in range(window_size)]\n",
    "            train_set.append((w2i[words[j]],[w2i[words[j-1]],w2i[words[j+1]]]))\n",
    "\n",
    "    print(\"Vocabulary size\")\n",
    "    print(len(w2i))\n",
    "    print()\n",
    "\n",
    "    #Training section\n",
    "    emb,_ = word2vec_trainer(train_set, len(w2i), freqtable, mode=mode, dimension=64, epoch=1, learning_rate=0.05)\n",
    "    \n",
    "    #Print similar words\n",
    "    testwords = [\"one\", \"are\", \"he\", \"have\", \"many\", \"first\", \"all\", \"world\", \"people\", \"after\"]\n",
    "    for tw in testwords:\n",
    "    \tsim(tw,w2i,i2w,emb)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
